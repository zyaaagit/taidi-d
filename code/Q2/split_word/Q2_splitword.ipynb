{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import json\n",
    "from pypinyin import lazy_pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件位置\n",
    "joblist_path = '../../../demo2/Q2_jobList.xlsx'\n",
    "resume_path = '../../../demo2/Q2_resume.xlsx'\n",
    "\n",
    "joblist_save_path = '../../../demo2/Q2_joblist(分词).xlsx'\n",
    "resume_save_path = '../../../demo2/Q2_resume(分词).xlsx'\n",
    "\n",
    "joblist_save_tolower_path = '../../../demo3/Q3_joblist.xlsx'\n",
    "resume_save_tolower_path = '../../../demo3/Q3_resume.xlsx'\n",
    "\n",
    "common_stopwords_path = 'stopwords/common_stopwords.txt'\n",
    "special_stopwords_path = 'stopwords/原数据中的停用词.txt'\n",
    "stopwords_path = 'stopwords/stop_words.txt'\n",
    "\n",
    "employee_dict_path = 'dict/employee_dict.txt'\n",
    "dict_path = 'dict/dict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词\n",
    "common_stopwords = open(common_stopwords_path,'r')\n",
    "special_stopwords = open(special_stopwords_path,'r')\n",
    "\n",
    "stopwords_common = []\n",
    "stopwords_special = []\n",
    "stop_words_list=[]\n",
    "\n",
    "for i in common_stopwords.readlines():\n",
    "    stopwords_common.append(i.strip())\n",
    "    stop_words_list.append(i.strip())\n",
    "for i in special_stopwords.readlines():\n",
    "    stopwords_special.append(i.strip())\n",
    "    stop_words_list.append(i.strip())\n",
    "with open(stopwords_path, 'w') as f:\n",
    "    for word in stop_words_list:\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取自定义词典\n",
    "# # 根据chatgpt得到分词后的文本-->(employee_dict)\n",
    "# employee_clean = pd.read_excel('emp_clean.xlsx')\n",
    "# employee_clean = employee_clean.replace(np.NAN,'')\n",
    "# employee_clean = employee_clean.replace('、',',', regex=True)\n",
    "# employee_dict = set()\n",
    "# for i in employee_clean.columns[1:-1]:\n",
    "#     for j in range(employee_clean.shape[0]):\n",
    "#         if employee_clean.loc[j,i]:\n",
    "#             for word in employee_clean.loc[j,i].split(','):\n",
    "#                 if word:\n",
    "#                     employee_dict.add(word)\n",
    "# employee_dict = list(employee_dict)\n",
    "# employee_dict.sort(key=lambda x: lazy_pinyin(x)[0][0])\n",
    "\n",
    "# # 将集合中的元素写入文本文件中\n",
    "# with open('dict/employee_dict.txt', 'w') as f:\n",
    "#     for word in employee_dict:\n",
    "#         if word:\n",
    "#             f.write(word + ' 10000'+ '\\n')\n",
    "\n",
    "# 人工过滤停用词-->(employee_dict_list)\n",
    "employee_dict = open(employee_dict_path,'r')\n",
    "employee_dict_list = [word.strip() for word in employee_dict.readlines()]\n",
    "employee_dict_list = [x for x in employee_dict_list if x != \"\"]\n",
    "employee_dict_list.sort(key=lambda x: lazy_pinyin(x)[0][0])\n",
    "with open(dict_path, 'w') as f:\n",
    "    for word in employee_dict_list:\n",
    "        if word:\n",
    "            f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清理文本中常见的六种特殊符号[,],【,】,',\",\",“,”\n",
    "def clean_data(data):\n",
    "    special_txt = r'\\[|\\]|【|】|\\'|\"|“|”'\n",
    "    data = data.replace(special_txt,'',regex=True)\n",
    "    data = data.replace('无','')\n",
    "    data = data.replace('未填写','')\n",
    "    data = data.replace(np.nan,'')\n",
    "    data = data.replace('无,无,无','')  \n",
    "    return data\n",
    "\n",
    "#分割列\n",
    "def split_column(data, column, split_columns_list):\n",
    "    a = 0\n",
    "    for _ in split_columns_list:\n",
    "        data[_] = data[column].map(lambda x:x.split(',')[a] if (x != '') else '')\n",
    "        a += 1\n",
    "    return data\n",
    "\n",
    "# 根据列、起始时间、新列名称计算时间间隔\n",
    "def time_count_days(data, row, start_time_column, end_time_column, new_column):\n",
    "    if data.loc[row, start_time_column] == '' or data.loc[row, end_time_column] == '':\n",
    "        return\n",
    "    else: \n",
    "        day = 0\n",
    "        for i in range(len(data.loc[row, end_time_column].split(','))):\n",
    "            a = datetime.strptime(data.loc[row, start_time_column].split(',')[i],\"%Y-%m-%d\")\n",
    "            b = datetime.strptime(data.loc[row, end_time_column].split(',')[i],\"%Y-%m-%d\")\n",
    "            day += (b-a).days\n",
    "        data.loc[row, new_column] = day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对resume文件进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/c3/vyxk7z2n5dx_dgz2fc4gsz440000gn/T/jieba.cache\n",
      "Loading model cost 0.542 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "resume_inf = pd.read_excel(resume_path, dtype = {'id':str})     #应聘人员信息\n",
    "\n",
    "resume_inf['id'] = resume_inf['id'].apply(lambda x:x.strip()) # 在jupyter上，\\t不一定代表制表符，也有可能是字符串\n",
    "resume_inf = clean_data(resume_inf)\n",
    "\n",
    "resume_jieba = ['selfEvaluation','workEP_descriptions','projectEp_descriptions','projectEP_achievements','trainEP_description']\n",
    "resume_jieba_newname = ['selfEvaluation_keywords','workEP_descriptions_keywords','projectEp_descriptions_keywords','projectEP_achievements_keywords','trainEP_description_keywords']\n",
    "\n",
    "jieba.load_userdict(dict_path) \n",
    "jieba.analyse.set_stop_words(stopwords_path)\n",
    "\n",
    "for i in range(len(resume_jieba)):\n",
    "    resume_inf[resume_jieba[i]] = resume_inf[resume_jieba[i]].map(lambda x:'' if x in stop_words_list else x)\n",
    "    resume_inf[resume_jieba_newname[i]] = resume_inf[resume_jieba[i]].map(lambda x:jieba.analyse.extract_tags(x,topK=20, allowPOS=('n|a|d|q|x')))\n",
    "    resume_inf[resume_jieba_newname[i]] = resume_inf[resume_jieba_newname[i]].astype(str)\n",
    "\n",
    "# 修改生日格式\n",
    "resume_inf['birthday'] = resume_inf['birthday'].apply(lambda x: x.strftime('%Y-%m-%d') if pd.isna(x) != True else '') #会自动将x转成str类型\n",
    "\n",
    "# 计算年龄\n",
    "today = date.today()\n",
    "resume_inf['age'] = resume_inf['birthday'].apply(lambda x: today.year - datetime.strptime(x,'%Y-%m-%d').year - ((today.month, today.day) < (datetime.strptime(x,'%Y-%m-%d').month, datetime.strptime(x,'%Y-%m-%d').day)) if x != '' else x)\n",
    "resume_inf['age'] = resume_inf['age'].apply(lambda x : str(x)+'岁')\n",
    "\n",
    "# 计算工作年限\n",
    "# 发现出现一个异常数据，在第115列，编号为1471800130456911872的廖女士的工作结束日期为2020年8月，早于开始日期2021年7月，因此认为是异常数据，将其手动改为2021年8月\n",
    "worktime_day = ['workEp_start','workEp_end']\n",
    "projecttime_day = ['projectEP_start','projectEP_end']\n",
    "traintime_day = ['trainEP_start','trainEP_end']\n",
    "edutime_day = ['eduEp_start','eduEp_end']\n",
    "days = ['worktime_days', 'projecttime_days', 'traintime_days', 'edutime_days']\n",
    "resume_inf.loc[115,worktime_day[1]] = '2021-08-27,2019-08-15'\n",
    "for i in range(resume_inf.shape[0]):\n",
    "    time_count_days(resume_inf, i, worktime_day[0], worktime_day[1], days[0])\n",
    "    time_count_days(resume_inf, i, projecttime_day[0], projecttime_day[1], days[1])\n",
    "    time_count_days(resume_inf, i, traintime_day[0], traintime_day[1], days[2])\n",
    "    time_count_days(resume_inf, i, edutime_day[0], edutime_day[1], days[3])\n",
    "resume_inf = clean_data(resume_inf) # 将np.nan转成''\n",
    "\n",
    "# 将时间转成int类型\n",
    "for i in days:\n",
    "    resume_inf[i] = resume_inf[i].apply(lambda x : int(x) if x != '' else x)\n",
    "\n",
    "# 计算是否应届\n",
    "resume_inf['is_graduate'] = resume_inf['eduEp_end'].apply(lambda x: '应届' if '2023'in x else '非应届' if len(x) != 0 else '未知')\n",
    "\n",
    "# 计算最高学历\n",
    "# 定义学历列表\n",
    "education_list = ['大专', '本科', '硕士', '博士']\n",
    "def highest_education(x):\n",
    "    if x == '':\n",
    "        return ''\n",
    "    else:\n",
    "        # 定义最高学历和最高学历索引\n",
    "        highest_education = ''\n",
    "        highest_index = -1\n",
    "        split_education = x.split(',')\n",
    "        # 遍历学历列表，查找最高学历\n",
    "        for edu in split_education:\n",
    "            # 查找学历在学历列表中的索引\n",
    "            index = education_list.index(edu)\n",
    "            if index > highest_index:\n",
    "                highest_index = index\n",
    "                highest_education = edu\n",
    "        return highest_education\n",
    "resume_inf['highest_education'] = resume_inf['eduEp_educationBackgrounds'].apply(highest_education)\n",
    "# 分割省市区\n",
    "city_split = ['province', 'county', 'region']\n",
    "resume_inf = split_column(resume_inf, 'expectCity', city_split)\n",
    "\n",
    "resume_inf = clean_data(resume_inf)\n",
    "# 导出数据集\n",
    "resume_inf.to_excel(resume_save_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_lower_column = ['expectPosition', 'trainEP_recordName', 'workEp_positionNames', 'keyword_info', 'lang_info', 'cert_info', 'skill_info', 'concatenated', 'selfEvaluation_keywords','workEP_descriptions_keywords','projectEp_descriptions_keywords','projectEP_achievements_keywords','trainEP_description_keywords']\n",
    "for i in resume_lower_column:\n",
    "    resume_inf[i] = resume_inf[i].str.lower()\n",
    "resume_inf.to_excel(resume_save_tolower_path,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对joblist_inf进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist_inf = pd.read_excel(joblist_path, dtype = {'id':str})\n",
    "joblist_inf['jobRequiredments_keywords'] = joblist_inf['jobRequiredments'].map(lambda x:jieba.analyse.extract_tags(x))\n",
    "joblist_inf['id'] = joblist_inf['id'].apply(lambda x:x.strip()) # 在jupyter上，\\t不一定代表制表符，也有可能是字符串\n",
    "joblist_inf.to_excel(joblist_save_path,index=False)\n",
    "joblist_inf = pd.read_excel(joblist_save_path, dtype = {'id':str})\n",
    "joblist_inf = clean_data(joblist_inf)\n",
    "joblist_inf.to_excel(joblist_save_path,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现了bug，只有先保存joblist，再读取joblist，才能对这个df里面的特殊符号进行去除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist_lower_column = ['positionName', 'keyword_info', 'skill_info', 'concatenated', 'jobRequiredments_keywords']\n",
    "for i in joblist_lower_column:\n",
    "    joblist_inf[i] = joblist_inf[i].str.lower()\n",
    "joblist_inf.to_excel(joblist_save_tolower_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
